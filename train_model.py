# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RuIjIfKA8zgAh5YlzMhHjp2ACA-Dajrh
"""









# Manipulation de donn√©es
import pandas as pd
import numpy as np

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns

# Mod√®les de Machine Learning
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from imblearn.ensemble import BalancedRandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# Deep Learning (optionnel si tu veux utiliser Keras/TensorFlow)


# Pr√©traitement et pipeline
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate
from sklearn.preprocessing import MinMaxScaler
from imblearn.combine import SMOTEENN
from imblearn.pipeline import Pipeline as ImbPipeline

# √âvaluation et m√©triques
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, mean_absolute_error, mean_squared_error,
    classification_report, confusion_matrix, roc_curve,
    precision_recall_curve, ConfusionMatrixDisplay, make_scorer
)

# Optimisation
import optuna
from optuna.samplers import TPESampler, CmaEsSampler, QMCSampler

# Sauvegarde / chargement
import joblib

import pandas as pd

# ID de ton fichier Google Drive (depuis le lien que tu as donn√©)
file_id = "1QYe6dmwvmz73PaYVZ9VV1TnBDtl7C9i1"

# URL de t√©l√©chargement direct
url = f"https://drive.google.com/uc?export=download&id={file_id}"

# Charger le CSV depuis Google Drive
df = pd.read_csv("telco_churn_preprocessed.csv")
# V√©rifier
print("Shape du dataset :", df.shape)
print(df.head())

# =======================================================
# 1. Pr√©paration des donn√©es
# =======================================================
print("üöÄ Pr√©paration des donn√©es...")
try:
    df = pd.read_csv("telco_churn_preprocessed.csv")
except FileNotFoundError:
    print("Erreur: Le fichier 'telco_churn_preprocessed.csv' n'a pas √©t√© trouv√©.")
    exit()

X = df.drop("Churn", axis=1)
y = df["Churn"]

# Normalisation des features
scaler = MinMaxScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# S√©paration des ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# R√©√©quilibrage des donn√©es d'entra√Ænement avec SMOTEENN
smoten = SMOTEENN(random_state=42)
X_train_res, y_train_res = smoten.fit_resample(X_train, y_train)

print("‚úÖ Donn√©es pr√™tes.")

# =======================================================
# 2. Entra√Ænement du mod√®le CatBoost avec les meilleurs param√®tres
# =======================================================
print("\nüß† Entra√Ænement du mod√®le CatBoost...")

# Les meilleurs param√®tres que vous avez d√©j√† fournis
best_catboost_params = {
    'iterations': 422,
    'depth': 12,
    'learning_rate': 0.04940621051278419,
    'l2_leaf_reg': 0.05491553217081499,
    'random_strength': 4.636852689665369,
    'grow_policy': 'SymmetricTree'
}

catboost_model = CatBoostClassifier(
    **best_catboost_params,
    logging_level="Silent",
    random_seed=42
)

catboost_model.fit(X_train_res, y_train_res)
print("‚úÖ Mod√®le entra√Æn√©.")

# =======================================================
# 3. Optimisation du seuil et √©valuation
# =======================================================
print("\nüìä √âvaluation des performances avec optimisation du seuil...")
y_prob = catboost_model.predict_proba(X_test)[:, 1]

# Optimisation du seuil pour maximiser le F1-score de la classe minoritaire
best_threshold = 0.5
best_f1_minority = 0
for threshold in np.arange(0.1, 0.9, 0.01):
    y_pred_temp = (y_prob >= threshold).astype(int)
    f1_minority = f1_score(y_test, y_pred_temp)
    if f1_minority > best_f1_minority:
        best_f1_minority = f1_minority
        best_threshold = threshold

y_pred_final = (y_prob >= best_threshold).astype(int)

# Affichage des m√©triques compl√®tes
print(f"Meilleur seuil de classification trouv√©: {best_threshold:.2f}")
print("\n--- Rapport de Classification ---")
print(classification_report(y_test, y_pred_final, zero_division=0))

print("\n--- M√©triques d'erreur et de pr√©cision ---")
mae = mean_absolute_error(y_test, y_pred_final)
mse = mean_squared_error(y_test, y_pred_final)
roc_auc = roc_auc_score(y_test, y_prob)

print(f"MAE : {mae:.4f}")
print(f"MSE : {mse:.4f}")
print(f"ROC AUC : {roc_auc:.4f}")

# Normalisation des features
scaler = MinMaxScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
print("‚úÖ Donn√©es pr√™tes.")

# =======================================================
# 2. D√©finition du pipeline et de la validation crois√©e
# =======================================================
print("\nüß† D√©finition du pipeline et de la strat√©gie de validation...")
best_catboost_params = {
    'iterations': 422,
    'depth': 12,
    'learning_rate': 0.04940621051278419,
    'l2_leaf_reg': 0.05491553217081499,
    'random_strength': 4.636852689665369,
    'grow_policy': 'SymmetricTree'
}

pipeline = ImbPipeline([
    ('smoteenn', SMOTEENN(random_state=42)),
    ('catboost', CatBoostClassifier(**best_catboost_params, logging_level="Silent", random_seed=42))
])

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
print("‚úÖ Pipeline et strat√©gie d√©finis.")

# =======================================================
# 3. √âvaluation par validation crois√©e (10 plis)
# =======================================================
print("\nüìä √âvaluation par validation crois√©e stratifi√©e (10 plis)...")
# Note : Pour les graphiques, on va r√©-entra√Æner sur tout l'ensemble de donn√©es.
print("‚úÖ √âvaluation termin√©e.")

# Re-entra√Ænement du mod√®le sur tout l'ensemble de donn√©es pour la pr√©diction finale
pipeline.fit(X, y)

# =======================================================
# 4. Pr√©diction et rapport final
# =======================================================
print("\nüéØ Rapport final du mod√®le entra√Æn√© sur toutes les donn√©es...")
y_prob_final = pipeline.predict_proba(X)[:, 1]

# Optimisation du seuil pour le meilleur F1-score macro
thresholds = np.arange(0.1, 0.9, 0.01)
best_threshold = 0.5
best_f1_macro = 0
for threshold in thresholds:
    y_pred_temp = (y_prob_final >= threshold).astype(int)
    f1_macro_temp = f1_score(y, y_pred_temp, average='macro', zero_division=0)
    if f1_macro_temp > best_f1_macro:
        best_f1_macro = f1_macro_temp
        best_threshold = threshold

y_pred_final = (y_prob_final >= best_threshold).astype(int)

# =======================================================
# 4. Pr√©diction et rapport final (ajout visualisations)
# =======================================================
print("\nüéØ Rapport final du mod√®le entra√Æn√© sur toutes les donn√©es...")

# Probabilit√©s
y_prob_final = pipeline.predict_proba(X)[:, 1]

# Optimisation du seuil
thresholds = np.arange(0.1, 0.9, 0.01)
best_threshold = 0.5
best_f1_macro = 0
for threshold in thresholds:
    y_pred_temp = (y_prob_final >= threshold).astype(int)
    f1_macro_temp = f1_score(y, y_pred_temp, average='macro', zero_division=0)
    if f1_macro_temp > best_f1_macro:
        best_f1_macro = f1_macro_temp
        best_threshold = threshold

y_pred_final = (y_prob_final >= best_threshold).astype(int)

# Affichage des m√©triques
print(f"\nMeilleur seuil de classification trouv√©: {best_threshold:.2f}")
print("\n--- Rapport de Classification ---")
print(classification_report(y, y_pred_final, zero_division=0))

print("\n--- M√©triques d'erreur et de pr√©cision ---")
mae = mean_absolute_error(y, y_pred_final)
mse = mean_squared_error(y, y_pred_final)
roc_auc = roc_auc_score(y, y_prob_final)

print(f"MAE : {mae:.4f}")
print(f"MSE : {mse:.4f}")
print(f"ROC AUC : {roc_auc:.4f}")

# =======================================================
# üîç Visualisations
# =======================================================

# 1. Matrice de confusion
cm = confusion_matrix(y, y_pred_final)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
            xticklabels=["Classe 0", "Classe 1"],
            yticklabels=["Classe 0", "Classe 1"])
plt.title("Matrice de confusion")
plt.xlabel("Pr√©dictions")
plt.ylabel("R√©el")
plt.show()

# =======================================================
# Matrice de confusion sur le jeu de test
# =======================================================
print("\nüìå Matrice de confusion sur le jeu de test :")

# Pr√©dictions sur le test
y_prob_test = pipeline.predict_proba(X_test)[:, 1]
y_pred_test = (y_prob_test >= best_threshold).astype(int)

# Matrice de confusion brute
cm_test = confusion_matrix(y_test, y_pred_test)
print(cm_test)

# Heatmap avec Seaborn
plt.figure(figsize=(6,5))
sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues", cbar=False,
            xticklabels=["Classe 0 (pr√©dit)", "Classe 1 (pr√©dit)"],
            yticklabels=["Classe 0 (r√©el)", "Classe 1 (r√©el)"])
plt.title("üß© Matrice de confusion (jeu de test)")
plt.xlabel("Pr√©dictions")
plt.ylabel("V√©rit√©s terrain")
plt.show()

# Alternative officielle scikit-learn
disp = ConfusionMatrixDisplay(confusion_matrix=cm_test,
                              display_labels=["Classe 0", "Classe 1"])
disp.plot(cmap="Blues", values_format="d")
plt.title("üß© Matrice de confusion (jeu de test)")
plt.show()

# 2. Courbe ROC
fpr, tpr, _ = roc_curve(y, y_prob_final)
plt.figure(figsize=(7,6))
plt.plot(fpr, tpr, label=f"ROC AUC = {roc_auc:.3f}")
plt.plot([0,1], [0,1], linestyle="--", color="gray")
plt.xlabel("Taux de faux positifs (FPR)")
plt.ylabel("Taux de vrais positifs (TPR)")
plt.title("Courbe ROC")
plt.legend()
plt.show()

# 3. Courbe Precision-Recall
prec, rec, _ = precision_recall_curve(y, y_prob_final)
plt.figure(figsize=(7,6))
plt.plot(rec, prec, label="Courbe Precision-Recall")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Courbe Precision-Recall")
plt.legend()
plt.show()

# 4. Histogramme des probabilit√©s pr√©dites
plt.figure(figsize=(7,5))
sns.histplot(y_prob_final[y==0], color="red", label="Classe 0", kde=True, bins=30)
sns.histplot(y_prob_final[y==1], color="blue", label="Classe 1", kde=True, bins=30)
plt.axvline(x=best_threshold, color="black", linestyle="--", label=f"Seuil {best_threshold:.2f}")
plt.xlabel("Probabilit√©s pr√©dites")
plt.ylabel("Fr√©quence")
plt.title("Distribution des probabilit√©s par classe")
plt.legend()
plt.show()

# 5. Barres comparatives des m√©triques
metrics = {
    "Accuracy": accuracy_score(y, y_pred_final),
    "Precision": precision_score(y, y_pred_final),
    "Recall": recall_score(y, y_pred_final),
    "F1 Macro": f1_score(y, y_pred_final, average="macro"),
    "ROC AUC": roc_auc
}

plt.figure(figsize=(8,5))
sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette="viridis")
plt.ylim(0,1)
plt.ylabel("Score")
plt.title("Comparaison des m√©triques")
for i, val in enumerate(metrics.values()):
    plt.text(i, val+0.02, f"{val:.3f}", ha="center")
plt.show()
joblib.dump((pipeline, X.columns.tolist()), "catboost_churn.pkl")
print("‚úÖ Mod√®le et colonnes sauvegard√©s : catboost_churn.pkl")